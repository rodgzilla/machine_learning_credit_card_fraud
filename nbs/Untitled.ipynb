{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from keras.models            import Sequential\n",
    "from keras.layers            import Input\n",
    "from keras.layers            import Dense\n",
    "from keras.layers            import BatchNormalization\n",
    "from keras.layers            import Activation\n",
    "from keras.layers            import Dropout\n",
    "from keras.utils.np_utils    import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm                    import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_std_df = pd.read_csv(data_path + 'creditcard.csv')\n",
    "non_std_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features    = non_std_df.drop(['Class'], axis = 1)\n",
    "target      = non_std_df['Class']\n",
    "scaler      = StandardScaler()\n",
    "features[:] = scaler.fit_transform(features)\n",
    "features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(df['Class'] == 0).sum(), (df['Class'] == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler_file = open(data_path + 'scaler.pkl', 'wb')\n",
    "pickle.dump(scaler, scaler_file)\n",
    "scaler_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.concat([features, target], axis = 1)\n",
    "df.to_csv(data_path + 'standardized_credit_card.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler_file = open(data_path + 'scaler.pkl', 'rb')\n",
    "scaler      = pickle.load(scaler_file)\n",
    "df          = pd.read_csv(data_path + 'standardized_credit_card.csv')\n",
    "scaler_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_training_batch(big_class, small_class, final_proportion):\n",
    "    batch_size              = int(round(big_class.shape[0] / (1 - final_proportion)))\n",
    "    print('batch size', batch_size)\n",
    "    small_class_sample_size = int(round(batch_size * final_proportion))  \n",
    "    print('small_class_sample_size', small_class_sample_size)\n",
    "    oversampled_small       = small_class.sample(small_class_sample_size, replace = True)\n",
    "    \n",
    "    return pd.concat([big_class, oversampled_small])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_to_numpy_arrays(df, test_prop):\n",
    "    # regular transaction df\n",
    "    reg_df   = df[df['Class'] == 0]\n",
    "    reg_feat = reg_df.drop(['Class'], axis = 1)\n",
    "    reg_targ = reg_df['Class']\n",
    "\n",
    "    # anomalies transaction df\n",
    "    anom_df   = df[df['Class'] == 1]\n",
    "    anom_feat = anom_df.drop(['Class'], axis = 1)\n",
    "    anom_targ = anom_df['Class']\n",
    "\n",
    "    # splitting\n",
    "    reg_feat_train, reg_feat_test, reg_targ_train, reg_targ_test     = train_test_split(reg_feat, reg_targ, test_size = test_prop)\n",
    "    anom_feat_train, anom_feat_test, anom_targ_train, anom_targ_test = train_test_split(anom_feat, anom_targ, test_size = test_prop)\n",
    "    \n",
    "    train_feat   = pd.concat([reg_feat_train, anom_feat_train]).as_matrix()\n",
    "    train_target = to_categorical(pd.concat([reg_targ_train, anom_targ_train]).as_matrix())\n",
    "    test_feat    = pd.concat([reg_feat_test, anom_feat_test]).as_matrix()\n",
    "    test_target  = to_categorical(pd.concat([reg_targ_test, anom_targ_test]).as_matrix())\n",
    "\n",
    "    return train_feat, train_target, test_feat, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, df, step_number, epoch_start, epoch_end, fit_params, validation_prop):\n",
    "    small_class_prop = df[df['Class'] == 1].shape[0] / df.shape[0]\n",
    "    steps            = np.linspace(0.5, small_class_prop, step_number)\n",
    "    epochs           = np.linspace(epoch_start, epoch_end, step_number)\n",
    "    print(epochs)\n",
    "    big_class        = df[df['Class'] == 0]\n",
    "    small_class      = df[df['Class'] == 1]\n",
    "    \n",
    "    for step, epoch in tqdm(zip(steps, epochs)):\n",
    "        training_batch                   = generate_training_batch(big_class, small_class, step)\n",
    "        X_train, y_train, X_test, y_test = df_to_numpy_arrays(training_batch, validation_prop)\n",
    "        fit_params['x']                  = X_train\n",
    "        fit_params['y']                  = y_train\n",
    "        fit_params['epochs']             = int(round(epoch))\n",
    "        fit_params['validation_data']    = (X_test, y_test)\n",
    "        print('Oversampling small class to %.2f%% of the training data, %d epochs' % (step, fit_params['epochs']))\n",
    "        model.fit(**fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(256, input_shape = (31,)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(256),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(256),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(256),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.50),\n",
    "    Dense(256),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.6),\n",
    "    Dense(256),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(2, activation = 'softmax')\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'batch_size': 2048,\n",
    "    'verbose': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.  5.]\n",
      "Oversampling small class to 0.50% of the training data, 5 epochs\n",
      "batch size 568630\n",
      "small_class_sample_size 284315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 483334 samples, validate on 85296 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3s - loss: 0.7590 - val_loss: 0.6911\n",
      "Epoch 2/5\n",
      "3s - loss: 0.6853 - val_loss: 0.6861\n",
      "Epoch 3/5\n",
      "2s - loss: 0.6771 - val_loss: 0.7346\n",
      "Epoch 4/5\n",
      "3s - loss: 0.6010 - val_loss: 1.9012\n",
      "Epoch 5/5\n"
     ]
    }
   ],
   "source": [
    "train_model(model, df, 2, 5, 5, fit_params, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "X           = df.drop(['Class'], axis = 1).as_matrix()\n",
    "y           = df['Class'].as_matrix()\n",
    "predictions = model.predict_classes(X, batch_size = 2048, verbose = 0)\n",
    "good_preds  = (predictions == y).sum()\n",
    "total_preds = X.shape[0]\n",
    "print('accuracy: %.2f' % (good_preds / total_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 147267, 1: 137540})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
